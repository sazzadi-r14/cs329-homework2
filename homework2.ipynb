{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T1M3qlQlhSu"
   },
   "source": [
    "## Stanford CS 329a Self-Improving AI Agents, Homework 2\n",
    "\n",
    "In this homework, our goal is to build an agentic workflow end to end by orchestrating LLM queries and augmenting them with API calls to ground the responses in factual information.\n",
    "\n",
    "The homework will have 4 parts.\n",
    "\n",
    "In the first part, we will measure the accuracy of an **API-augmented LLM pipeline** - we first augment the LLM with four different API calls, then implement an **API router**  (LLM-based) that routes the query to the appropriate API, and then generate the prompt from the API outputs and the input query.\n",
    "\n",
    "In the second part, we will use **self-improvement techniques** to improve the accuracy by using a) **query decomposition and fusion**, where we decompose the complex queries in to sub-queries, generate the prompts for each of the sub-queries, and then fuse the model outputs, and b) **iterative self-refinement**, where we use LLM as a judge to evaluate the model responses and if it is not satisfactory, we iterate by querying additional information from the APIs and improving the response.\n",
    "\n",
    "In the third part, we implement the end-to-end **agentic workflow** using the components implemented in first two parts and evaluate the accuracy on the full test set.\n",
    "\n",
    "In the fourth (bonus) part, we suggest using the components you implemented above to build a **deep research agent** for knowledge-intensive research tasks (similar to the recent launches from OpenAI and Google) and test this on a small set of queries.\n",
    "\n",
    "**Final Deliverable**: A zipped folder (.zip) of your fork of the HW#2 Github with your edited files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs9oF7xNlhSv"
   },
   "source": [
    "### Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w9Q5ptwolhSv"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip -q install google-api-python-client\n",
    "!{sys.executable} -m pip -q install textblob\n",
    "!{sys.executable} -m pip -q install python-dotenv\n",
    "!{sys.executable} -m pip -q install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vu1ST_gdlhSw"
   },
   "source": [
    "#### API Configuration and Setup\n",
    "\n",
    "Before we can use the APIs, we need to set up our environment and initialize the API manager. This involves:\n",
    "\n",
    "1. Loading API keys from environment variables\n",
    "2. Setting up the API manager with the necessary credentials\n",
    "3. Validating that all required keys are present\n",
    "\n",
    "For this homework, we'll use:\n",
    "- Google Custom Search API (requires API key and Custom Search Engine ID):\n",
    "     - https://developers.google.com/custom-search/v1/overview\n",
    "     - https://programmablesearchengine.google.com/controlpanel/create\n",
    "- Alpha Vantage API (requires API key):\n",
    "     - https://www.alphavantage.co/support/#api-key\n",
    "\n",
    "**Make sure to use your compute budget carefully!** Try to use smaller, cheaper LMs (e.g. llama 8B, gpt-4o-mini, claude-3-5-haiku-latest) as much as possible for development before switching to larger, more expensive models.\n",
    "\n",
    "**Note**: In practice, you should store your API keys in a `.env` file for security. For this homework, we're using placeholder keys for demonstration purposes. Please don't submit the API keys as part of your homework submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iwDagCRflhSw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from cs329_hw2.api_manager import APIManager\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "GOOGLE_CX_ID = os.getenv('GOOGLE_CX_ID')\n",
    "ALPHA_VANTAGE_KEY = os.getenv('ALPHA_VANTAGE_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "TOGETHER_API_KEY = os.getenv('TOGETHER_API_KEY')\n",
    "\n",
    "\n",
    "if not GOOGLE_API_KEY or not ALPHA_VANTAGE_KEY:\n",
    "    raise ValueError(\"Missing required API keys in environment variables\")\n",
    "\n",
    "# Initialize API manager with keys\n",
    "api_manager = APIManager(\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    google_cx_id=GOOGLE_CX_ID,\n",
    "    alpha_vantage_key=ALPHA_VANTAGE_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow3h0bBtlhSx"
   },
   "source": [
    "##  Part 0 - LLM performance with single call [5 points]\n",
    "\n",
    "The Generation component allows you to generate a response to the constructed prompt with a selected model. For different tasks, it can be useful to use different models to generate multiple solutions to the same query. This approach leverages the strengths of different models to produce varied perspectives and solutions.\n",
    "\n",
    "Key requirements:\n",
    "- Implement generation supporting multiple models (GPT-4o-mini, Claude Haiku, Llama 405B)\n",
    "- Handle generation parameters (temperature, max tokens)\n",
    "- Error handling for failed generations\n",
    "- Return structured responses with model attribution\n",
    "\n",
    "To show the limitations of using a single LM, let's test the performance of a single LM (with no API calls or systematic processing) on queries focused on agentic tasks and tool-use.\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `generate` method in the MultiLMAgent class. Then, compare how individual models perform on the same query before comparing to the performance of a multi-stage approach for queries.\n",
    "\n",
    "**Note**: A few questions to consider as you test your implementation:\n",
    "- How does the accuracy of the generated responses compare between different models?\n",
    "- How does the choice of model affect the quality and diversity of the responses?\n",
    "- Do all problems require multiple generations or can some be answered with a single generation?\n",
    "- How can we improve the capabilities of a single LM to answer these types of questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs329_hw2.multi_lm_agent import MultiLMAgent\n",
    "from cs329_hw2.api_manager import APIManager\n",
    "from cs329_hw2.evaluation import prepare_dataset, evaluate_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v45Wk6H5lhSx"
   },
   "outputs": [],
   "source": [
    "# Initialize the Multi-LM Agent\n",
    "multi_lm_agent = MultiLMAgent(api_manager=None)\n",
    "\n",
    "# In debug mode, we only load the first 10 rows of the dataset for development purposes.\n",
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode=debug_mode) \n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "print(\"Generated Responses with Different Models:\")\n",
    "openai_responses = []\n",
    "anthropic_responses = []\n",
    "llama_responses = []\n",
    "\n",
    "print(f\"Total queries to process: {len(queries)}\")\n",
    "\n",
    "for query, answer in zip(queries, answers):\n",
    "    openai_response = multi_lm_agent.generate(query, model=\"gpt-4o-mini\")\n",
    "    anthropic_response = multi_lm_agent.generate(query, model=\"claude-3-5-haiku-latest\")\n",
    "    llama_response = multi_lm_agent.generate(query, model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\")\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"OpenAI Response: {openai_response[:200]}...\\n\")\n",
    "    print(f\"Anthropic Response: {anthropic_response[:200]}...\\n\")\n",
    "    print(f\"Llama Response: {llama_response[:200]}...\\n\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    openai_responses.append(openai_response)\n",
    "    anthropic_responses.append(anthropic_response)\n",
    "    llama_responses.append(llama_response)\n",
    "\n",
    "# Evaluate accuracy for each model\n",
    "accuracy_openai, results_openai = evaluate_qa(queries, openai_responses, answers)\n",
    "accuracy_anthropic, results_anthropic = evaluate_qa(queries, anthropic_responses, answers)\n",
    "accuracy_llama, results_llama = evaluate_qa(queries, llama_responses, answers)\n",
    "\n",
    "print(results_openai)\n",
    "\n",
    "print(\"\\n\\n\\nAccuracy Results:\")\n",
    "print(f\"OpenAI GPT-4 Accuracy: {accuracy_openai}\")\n",
    "print(f\"Anthropic Claude Accuracy: {accuracy_anthropic}\")\n",
    "print(f\"Meta Llama Accuracy: {accuracy_llama}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE5jRypG1aEm"
   },
   "source": [
    "## Part 1 - API-augmented LLM pipeline [35 points]\n",
    "\n",
    "We augment the LLM with four different API calls, then implement an API router (LLM-based) that routes the query to the appropriate API, and then generate the prompt from the API outputs and the input query. We will work with the following APIs and select the appropriate API for a given query:\n",
    "\n",
    "1. **Google Custom Search API** - For web search capabilities\n",
    "2. **Alpha Vantage API** - For real-time financial data\n",
    "3. **Weather API** - For location-based weather forecasting\n",
    "4. **Analyze Sentiment API** - For sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQBW_rgJlhSw"
   },
   "source": [
    "#### 1a. Google Custom Search API [5 points]\n",
    "\n",
    "The Google Custom Search API allows us to programmatically search the web. We'll use this to gather information and context for our tasks.\n",
    "\n",
    "Key features:\n",
    "- Web search with customizable parameters\n",
    "- Filtering and sorting options\n",
    "- Rich metadata about search results\n",
    "\n",
    "Deliverable: In the ``cs329_hw2/api_manager.py`` file, implement the ``google_search`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KcoHwuClhSw"
   },
   "outputs": [],
   "source": [
    "search_query = \"Apple Product News\"\n",
    "results = api_manager.google_search(\n",
    "        search_term=search_query,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "print(\"\\nSearch Results for:\", search_query)\n",
    "print(\"-\" * 50)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Link: {result['link']}\")\n",
    "    print(f\"Snippet: {result['snippet']}\")\n",
    "    print(f\"API Content Formatted: {result['api_content']['formatted']}\")\n",
    "    print(f\"API Content Plain: {result['api_content']['plain']}\")\n",
    "    print(f\"Webpage Content: {result['webpage_content']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZH5yIpMlhSw"
   },
   "source": [
    "#### 1b. Alpha Vantage API [5 points]\n",
    "\n",
    "The Alpha Vantage API provides real-time and historical financial data. We'll use this for analyzing stock market information.\n",
    "\n",
    "Key features:\n",
    "- Real-time stock quotes\n",
    "- Historical price data\n",
    "- Technical indicators\n",
    "- Company fundamentals\n",
    "\n",
    "Deliverable: In the ``cs329_hw2/api_manager.py`` file, implement the ``get_stock_data`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cjmJmwmlhSw"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "stock_data = api_manager.get_stock_data(symbol=\"TSLA\", date=\"2025-01-22\")\n",
    "if isinstance(stock_data, dict):\n",
    "    print(\"\\nStock Information:\")\n",
    "    print(f\"Date: {stock_data['date']}\")\n",
    "    print(f\"Open: {stock_data['open']}\")\n",
    "    print(f\"High: {stock_data['high']}\")\n",
    "    print(f\"Low: {stock_data['low']}\")\n",
    "    print(f\"Close: {stock_data['close']}\")\n",
    "    print(f\"Volume: {stock_data['volume']}\")\n",
    "else:\n",
    "    print(\"No stock data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "196BcY7alhSw"
   },
   "source": [
    "#### 1c. Sentiment Analysis API [5 points]\n",
    "\n",
    "The Sentiment Analysis API helps determine the emotional tone and subjectivity of text content. This API processes text input and returns:\n",
    "\n",
    "Key features:\n",
    "- Overall sentiment classification (positive/negative/neutral)\n",
    "- Polarity score (-1 to 1, indicating how negative or positive)\n",
    "- Subjectivity measure (0 to 1, indicating how objective or subjective)\n",
    "- Support for multiple languages\n",
    "- Real-time text analysis\n",
    "\n",
    "The API is particularly useful for:\n",
    "- Customer feedback analysis\n",
    "- Social media monitoring\n",
    "- Product review assessment\n",
    "- Brand sentiment tracking\n",
    "\n",
    "Deliverable: In the ``cs329_hw2/api_manager.py`` file, implement the ``analyze_sentiment`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9COV1K0xlhSx"
   },
   "outputs": [],
   "source": [
    "# Example usage of sentiment analysis with API manager\n",
    "text = \"I really love this new phone! The camera quality is amazing.\"\n",
    "result = api_manager.analyze_sentiment(text)\n",
    "\n",
    "if result:\n",
    "    print(\"\\nSentiment Analysis:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['sentiment']}\")\n",
    "    print(f\"Polarity: {result['polarity']:.2f}\")\n",
    "    print(f\"Subjectivity: {result['subjectivity']:.2f}\")\n",
    "else:\n",
    "    print(\"No sentiment analysis result available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8ywSe9LlhSx"
   },
   "source": [
    "#### 1d. Weather API [5 points]\n",
    "\n",
    "The Weather API provides current and forecasted weather data for any location. We'll use this for location-based weather analysis.\n",
    "\n",
    "Key features:\n",
    "- Current weather conditions\n",
    "- Hourly and daily forecasts\n",
    "- Historical weather data\n",
    "- Various weather parameters (temperature, precipitation, wind, etc.)\n",
    "\n",
    "Deliverable: In the ``cs329_hw2/api_manager.py`` file, implement the ``get_weather`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nM0nrG0SlhSx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting weather for location: Palo Alto, CA, US, date: 2025-02-13, hour: 12\n",
      "\n",
      "Getting coordinates for location: Palo Alto, CA, US\n",
      "<Response [200]>\n",
      "[{'name': 'Palo Alto', 'local_names': {'es': 'Palo Alto', 'uz': 'Palo-Alto', 'hi': 'पालो आल्टो', 'ar': 'بالو ألتو', 'ko': '팰로앨토', 'he': 'פאלו אלטו', 'uk': 'Пало-Альто', 'ur': 'مئیفیلڈ', 'el': 'Πάλο Άλτο', 'zh': '帕羅奧圖', 'sr': 'Пало Алто', 'eo': 'Palo Alto', 'ru': 'Пало-Альто', 'nv': 'Tsin Łichíʼínééz', 'en': 'Palo Alto', 'bg': 'Пало Алто', 'lv': 'Paloalto', 'fa': 'پالو آلتو', 'ja': 'パロアルト'}, 'lat': 37.4443293, 'lon': -122.1598465, 'country': 'US', 'state': 'California'}]\n",
      "Geocoding API request successful\n",
      "Coordinates found - lat: 37.4443293, lon: -122.1598465\n",
      "Coordinates retrieved - lat: 37.4443293, lon: -122.1598465\n",
      "Making API request to OpenWeather...\n",
      "Error fetching weather data: 401 Client Error: Unauthorized for url: https://api.openweathermap.org/data/3.0/onecall?lat=37.4443293&lon=-122.1598465&exclude=minutely%2Chourly%2Cdaily%2Calerts&appid=42a9cd5b84580fd8eedf134fa225bc4c&units=metric\n",
      "weather_data {'error': 'Failed to retrieve weather data'}\n",
      "\n",
      "Weather Conditions for Palo Alto, CA, US on 2025-02-13 at 12:00:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'weather_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weather_data:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWeather Conditions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhour\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:00:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeather: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mweather_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweather_description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweather_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m°C\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWind Speed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweather_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_speed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m km/h\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'weather_description'"
     ]
    }
   ],
   "source": [
    "# We can use the API manager's weather functionality\n",
    "from datetime import datetime\n",
    "from cs329_hw2.api_manager import APIManager\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "hour = 12  # 12:00 (noon)\n",
    "location = \"Palo Alto, CA, US\"\n",
    "\n",
    "# Get weather data using API manager\n",
    "weather_data = api_manager.get_weather(location, current_date, hour)\n",
    "print(\"weather_data\", weather_data)\n",
    "\n",
    "\n",
    "if weather_data:\n",
    "    print(f\"\\nWeather Conditions for {location} on {current_date} at {hour}:00:\")\n",
    "    print(f\"Weather: {weather_data['weather_description']}\")\n",
    "    print(f\"Temperature: {weather_data['temperature']}°C\")\n",
    "    print(f\"Wind Speed: {weather_data['wind_speed']} km/h\")\n",
    "    print(f\"Conditions: {weather_data['weather_description']}\")\n",
    "else:\n",
    "    print(f\"Could not get weather data for {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj37BiAIlhSx"
   },
   "source": [
    "#### 1e. API Routing [10 points]\n",
    "\n",
    "The API routing is a system that uses language models to route queries to the appropriate API function. This allows us to build an agent that can use multiple APIs to answer user queries.\n",
    "\n",
    "Key requirements for the function logic and prompt construction:\n",
    "- Query an LLM to determine the appropriate API to use for the query\n",
    "- Correctly parse the query response and map it to the appropriate API function\n",
    "- In the query response, include the API name, parameters to be used, and the order of execution\n",
    "- Query the selected API and return the response from the API after parsing\n",
    "- Handle edge cases and fallbacks for query parsing and API selection\n",
    "\n",
    "Deliverable: In the ``cs329_hw2/api_manager.py`` file, implement the ``parse_query_params`` and ``route_query`` functions.\n",
    "\n",
    "Note: OpenAI has the structured output support, which can simplify the implementation of these functions: https://platform.openai.com/docs/guides/structured-outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LxUoUTdlhSx"
   },
   "outputs": [],
   "source": [
    "# Test queries for each API category\n",
    "queries = [\n",
    "    # Google Search queries\n",
    "    \"What were the key announcements from Google I/O 2024?\",\n",
    "    \"What are the opening hours of The French Laundry restaurant in Yountville?\",\n",
    "\n",
    "    # Stock Data queries\n",
    "    \"What's the change in NVIDIA (NVDA) stock on 2024-01-22?\",\n",
    "    \"Show me the daily percentage change for Amazon (AMZN) shares\",\n",
    "\n",
    "    # Sentiment Analysis queries\n",
    "    \"The new iOS update completely broke my phone's battery life and I'm furious about having to charge it three times a day!\",\n",
    "    \"Despite minor delays, the conference exceeded expectations with groundbreaking research presentations and networking opportunities.\",\n",
    "\n",
    "    # Weather queries\n",
    "    \"What's the expected wind speed and precipitation in Chicago this Friday, 2025-01-24?\",\n",
    "    \"Was there any thunderstorms in Atlanta, GA last year on December 21st, 2024?\",\n",
    "]\n",
    "\n",
    "# Test each query\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    output = api_manager.route_query(query)\n",
    "    print(output)\n",
    "    print(\"Result:\", output[\"results\"])\n",
    "    print(\"API Used:\", output[\"api_used\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8SAOb3c39fg"
   },
   "source": [
    "#### 1f. LLM evaluation with single LM and with API routing [5 points]\n",
    "\n",
    "With the added API calls, we can now evaluate the performance of the single LM and the API routing. Create a simple function that takes in a query and model choice, retrieves necessary data from the API manager, prompts the model with the query and the data, and returns the response from the model.\n",
    "\n",
    "Key requirements for the function prompt and logic:\n",
    "- Query the API manager for the necessary data\n",
    "- Use the query and the data retrieved from the API manager to create a prompt for the model\n",
    "- Use the model to generate the response\n",
    "- Return the response from the model\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `single_LM_with_single_API_call` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDW-NL39lhSy"
   },
   "outputs": [],
   "source": [
    "# Without multi-LM system: How well does a single-call, single LM perform?\n",
    "from cs329_hw2.evaluation import evaluate_qa\n",
    "\n",
    "# Initialize the Multi-LM Agent\n",
    "multi_lm_agent = MultiLMAgent(api_manager)\n",
    "# In debug mode, we only load the first 10 rows of the dataset for development purposes.\n",
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode)\n",
    "\n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "print(\"Generated Responses with Different Models:\")\n",
    "single_LM_with_single_API_call_responses = []\n",
    "for query, answer in zip(queries, answers):\n",
    "    openai_response = multi_lm_agent.generate(query, model=\"gpt-4o-mini\")\n",
    "    single_LM_with_single_API_call_responses.append(openai_response)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Single-Call LM Response: {openai_response[:200]}...\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Evaluate the single LM call, single API call performance\n",
    "# This will be used later in the notebook to visualize the performance of the single LM with a single API call\n",
    "accuracy_singleLM_with_single_API_call, results_singleLM_with_single_API_call = evaluate_qa(queries, single_LM_with_single_API_call_responses, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ9lEYYe4ilL"
   },
   "source": [
    "## Part 2 - Self-improvement techniques [40 points]\n",
    "We now improve the accuracy by using a) **query decomposition and fusion** and b) **iterative self-refinement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_ctH4gJlhSx"
   },
   "source": [
    "#### 2a. Query Decomposition [10 points]\n",
    "\n",
    "To handle more complex queries, we will create a series of components that can be used to breakdown and execute on subtasks. First, we will create the Query Decomposition component, which breaks down complex queries into simpler, more manageable parts. This allows us to use multiple APIs to answer the query.\n",
    "\n",
    "Key requirements for prompt construction:\n",
    "- Use the LLM to separate the query into multiple sub-queries relevant for answering the original query\n",
    "- Query the API manager for the necessary data for each sub-query\n",
    "- Gather the API results from each sub-query\n",
    "- Return the gathered structured results with API attribution and variables\n",
    "- Error handling for failed decompositions, failed API calls, and failed query parsing\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `decompose_query` method in the MultiLMAgent class.\n",
    "\n",
    "**Note**: The `decompose_query` method should take a query and return a list of sub-queries. How do these sub-queries help with the overall task? What information do they provide that the original query does not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRXAturVlhSx"
   },
   "outputs": [],
   "source": [
    "from cs329_hw2.multi_lm_agent import MultiLMAgent\n",
    "from cs329_hw2.api_manager import APIManager\n",
    "from cs329_hw2.evaluation import prepare_dataset\n",
    "\n",
    "# Initialize the Multi-LM Agent\n",
    "multi_lm_agent = MultiLMAgent(api_manager)\n",
    "\n",
    "# Test Query Decomposition\n",
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode) # In debug mode, load the first 10 rows of the dataset for development purposes.\n",
    "\n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "print(\"Testing Query Decomposition:\")\n",
    "for query in queries:\n",
    "    print(f\"\\nOriginal Query: {query}\")\n",
    "    decomposed = multi_lm_agent.decompose_query(query=query)\n",
    "\n",
    "    print(\"Sub-queries:\")\n",
    "    for sub_query in decomposed:\n",
    "        print(f\"{sub_query}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY4wfZGYlhSy"
   },
   "source": [
    "#### 2b. Prompt construction for sub-queries [5 points]\n",
    "\n",
    "To handle the decomposed sub-queries, the Prompt Construction component constructs prompts for each sub-query. This allows us to use multiple APIs to answer the query.\n",
    "\n",
    "Key requirements for prompt construction:\n",
    "- Implement prompt construction to combine the sub-queries and their results into a single prompt\n",
    "- Return structured responses with model attribution\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `generate_prompt` method in the MultiLMAgent class.\n",
    "\n",
    "**Note**: Do the prompts constructed provide all the necessary information to answer the query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKJZWHRVlhSy"
   },
   "outputs": [],
   "source": [
    "# Test Query Decomposition\n",
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode) # In debug mode, load the first 10 rows of the dataset for development purposes.\n",
    "\n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "for query in queries:\n",
    "\n",
    "    decomposed_queries = multi_lm_agent.decompose_query(query)\n",
    "    generated_prompt = multi_lm_agent.generate_prompt(query, decomposed_queries)\n",
    "\n",
    "    print(\"Generated Prompt:\")\n",
    "    print(generated_prompt)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRYpZTtMlhSy"
   },
   "source": [
    "#### 2c. Fusion of sub-query responses [5 points]\n",
    "\n",
    "With the constructed prompt, we can generate multiple responses with different models to answer the query, getting a diversity of responses. With these varied answers, the Fusion component combines the best elements from separate responses into a single coherent output. This creates a more comprehensive and accurate response than any single model could provide.\n",
    "\n",
    "Key requirements for prompt construction:\n",
    "- Call the query decomposition and prompt construction functions to get the decomposed queries and the generated prompt\n",
    "- Query multiple models with the generated prompt (specifically \"gpt-4o-mini\", \"claude-3-5-haiku-latest\", and \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\")\n",
    "- Combine elements from multiple responses by creating a new prompt for fusion \n",
    "- Maintain consistency and clarity in the final response\n",
    "- Handle edge cases and fallbacks\n",
    "- Return the final response from the fusion model\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `fuse` method in the MultiLMAgent class. This function should use the `generate`, `decompose_query`, and `generate_prompt` methods.\n",
    "\n",
    "**Note**: The `fuse` method should take the generated prompt and multiple generated responses before returning a single fused response. Compare how the fused response is different from the individual responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixZRAS2VlhSy"
   },
   "outputs": [],
   "source": [
    "### With query decomposition, prompt construction, and fusion: How well does a multi-call, tool-augmented LM perform?\n",
    "\n",
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode) # In debug mode, load the first 10 rows of the dataset for development purposes.\n",
    "\n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "print(\"Generated Responses with Multi-LM System:\")\n",
    "multi_LM_responses = []\n",
    "for query, answer in zip(queries, answers):\n",
    "    fused_response = multi_lm_agent.fuse(query)\n",
    "    multi_LM_responses.append(fused_response)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Fusion Response: {fused_response[:200]}...\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Evaluate the multi-call, tool-augmented LM performance\n",
    "# This will be used later in the notebook to visualize the performance of the single-call, multi-LM system\n",
    "accuracy_querydecomposition, results_querydecomposition = evaluate_qa(queries, multi_LM_responses, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MeFCmOplhSy"
   },
   "source": [
    "#### 2d. Iterative Self-Refinement [20 points]\n",
    "\n",
    "Sometimes a single round of decomposition, API calls, and fusion is not enough to answer the query. To address this issue, the Iterative Refinement component iteratively improves the response by querying for more information, as needed, using multiple APIs. This can be particularly useful for complex queries that require multiple API calls to answer, such as multi-hop question-answering or agentic queries that require multiple steps to answer.\n",
    "\n",
    "Key requirements for function logic and prompt construction:\n",
    "- Generate response(s) to the query using one or more models, additional API calls, and maximum query budget\n",
    "- This function should use all of the previous functions implemented: api routing, decomposition, prompt construction, and fusion\n",
    "- Query the LLM to evaluate the response and repeat the loop of query decomposition for API calls, prompt construction, and fusion, if needed\n",
    "- Exit when the response is satisfactory or the maximum number of iterations is reached\n",
    "\n",
    "For resources on LM judges and self-verification, see: \n",
    "- [LM Judge Survey](https://arxiv.org/abs/2411.15594)\n",
    "- [Large Language Models are Better Reasoners with Self-Verification](https://arxiv.org/abs/2212.09561)\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `iterative_refine` method in the MultiLMAgent\n",
    "class.\n",
    "\n",
    "**Note**: For now, make sure to specify just two rounds of iterative refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zws1bpWplhSy"
   },
   "outputs": [],
   "source": [
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode)\n",
    "\n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "print(\"Generated Responses with Iterative Refinement:\")\n",
    "iterative_refinement_responses = []\n",
    "for query, answer in zip(queries, answers):\n",
    "    final_response = multi_lm_agent.iterative_refine(query, model=\"gpt-4o-mini\", max_iterations=2)\n",
    "    iterative_refinement_responses.append(final_response)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {final_response[:200]}...\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Evaluate the multi-call, tool-augmented LM performance\n",
    "# This will be used later in the notebook to visualize the performance of the multi-call, multi LM system\n",
    "accuracy_iterative_refinement, iterative_refinement_results = evaluate_qa(queries, iterative_refinement_responses, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps8Kh6HdlhSy"
   },
   "source": [
    "Now let's see how adding more iterations of refinement improves the performance! Note that performance should improve as we add more iterations, but it should not improve indefinitely.\n",
    "\n",
    "Explore your implementation with different models and different query types to see how the performance changes! See if there is an optimal number of iterations for different query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4wP21Y9lhSy"
   },
   "outputs": [],
   "source": [
    "debug_mode = True\n",
    "dataset = prepare_dataset(debug_mode)\n",
    "\n",
    "queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "print(\"Generated Responses with 5 rounds ofIterative Refinement:\")\n",
    "iterative_refinement_responses_more_rounds = []\n",
    "for query, answer in zip(queries, answers):\n",
    "    final_response = multi_lm_agent.iterative_refine(query, model=\"gpt-4o-mini\", max_iterations=5)\n",
    "    iterative_refinement_responses_more_rounds.append(final_response)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {final_response[:200]}...\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Evaluate the multi-call, tool-augmented LM performance with more rounds of refinement\n",
    "# This will be used later in the notebook to visualize the performance of the multi-call, multi LM system\n",
    "accuracy_multi_round_iterative_refinement, multi_round_iterative_refinement_results = evaluate_qa(queries, iterative_refinement_responses_more_rounds, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bar graph plotting the accuracies of single-call LM, single-call LM with API routing, iterative refinement with 2 maximum rounds of iterative refinement, and iterative refinement with 5 maximum rounds of iterative refinement on the debug dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJKaRSYGlhSy"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create data for the bar graph\n",
    "methods = ['Single-Call LM', 'Single-Call w/API', 'Iterative (2 rounds)', 'Iterative (5 rounds)']\n",
    "\n",
    "# Must be in this order and in floating point format from 0.0 to 1.0\n",
    "accuracies = [\n",
    "    accuracy_singleLM,\n",
    "    accuracy_querydecomposition, \n",
    "    accuracy_iterative_refinement,\n",
    "    accuracy_multi_round_iterative_refinement\n",
    "]\n",
    "\n",
    "# Define colors for each bar\n",
    "colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99']\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(methods, accuracies, color=colors)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Accuracy Comparison Across Different LM Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.0)  # Set y-axis from 0 to 1 since these are accuracy values\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2%}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBNcopTVlhSy"
   },
   "source": [
    "## Part 3 - Building an LLM Agentic Workflow [20 points]\n",
    "\n",
    "Using the components we've built (or new ones you've implemented), build an LLM agent that can answer the following questions about the world. These questions will be a mix of the types of questions we've built components for and will require using the APIs in creative ways.\n",
    "\n",
    "Some questions will require just a single API call, while others will require multiple API calls and multiple rounds of iterative refinement. Create a pipeline that can dynamically adjust to the complexity of the question. Feel free to implement new components or use the ones we've already built!\n",
    "\n",
    "If you get above 70% accuracy on the test set, you will get full points. For scores below 70%, you will get partial credit based on the percentage of accuracy. With each of the components currently implemented, the agent should be capable of getting above 70% accuracy on the test set.\n",
    "\n",
    "**Important**: Make sure to evaluate over the entire dataset when you are confident with your implementation! This will help you preserve inference compute credits and speed up the development process.\n",
    "\n",
    "Deliverable: In the `cs329_hw2/multi_lm_agent.py` file, implement the `run_pipeline` method in the MultiLMAgent class. It should take a query and return a response. Furthermore, it should be able to handle all the queries outlined below before the responses are evaluated.\n",
    "\n",
    "**Note**: How does the performance on the dataset compare between single-call LMs vs. the complete pipeline with the multi-LM agent? How does it improve accuracy by improving access to tool APIs and allowing for more complex reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first evaluate over the entire dataset with just a single model\n",
    "- **IMPORTANT**: Make sure to evaluate over the entire dataset when you are confident with your implementation! \n",
    "- This will help you preserve inference compute credits and speed up the development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MT5JPpsxlhSz"
   },
   "outputs": [],
   "source": [
    "# Let's first evaluate over the entire dataset with just a single model\n",
    "# IMPORTANT: Make sure to evaluate over the entire dataset when you are confident with your implementation! \n",
    "# This will help you preserve inference compute credits and speed up the development process.\n",
    "\n",
    "from cs329_hw2.evaluation import evaluate_qa\n",
    "\n",
    "debug_mode = False # Loads the entire test dataset for evaluation.\n",
    "dataset = prepare_dataset(debug_mode)\n",
    "\n",
    "test_queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "single_model_responses = []\n",
    "for query, answer in zip(test_queries, answers):\n",
    "    response = multi_lm_agent.generate(query, model=\"gpt-4o-mini\")\n",
    "    single_model_responses.append(response)\n",
    "\n",
    "print(\"Evaluating single-model performance...\")\n",
    "complete_set_single_model_accuracy, complete_set_single_model_results = evaluate_qa(test_queries, single_model_responses, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate over the entire dataset with our multi-LM agentic pipeline\n",
    "- **IMPORTANT**: Make sure to evaluate over the entire dataset when you are confident with your implementation! \n",
    "- This will help you preserve inference compute credits and speed up the development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taP3V7VFlhSz"
   },
   "outputs": [],
   "source": [
    "from cs329_hw2.evaluation import evaluate_qa\n",
    "\n",
    "debug_mode = False # Loads the entire test dataset for evaluation.\n",
    "dataset = prepare_dataset(debug_mode)\n",
    "\n",
    "test_queries = dataset['train']['query']\n",
    "answers = dataset['train']['answer']\n",
    "\n",
    "multi_lm_agent = MultiLMAgent(api_manager,\n",
    "                              decomposition_model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "                              iterative_refinement_model=\"claude-3-5-haiku-latest\",\n",
    "                              fusion_model=\"gpt-4o-mini\")\n",
    "\n",
    "multi_lm_responses = []\n",
    "print(\"\\nGenerating responses with Multi-LM Agent...\")\n",
    "for query, answer in zip(test_queries, answers):\n",
    "    response = multi_lm_agent.run_pipeline(query)\n",
    "    multi_lm_responses.append(response)\n",
    "\n",
    "print(\"Evaluating multi-LM agent performance...\")\n",
    "complete_set_agentic_accuracy, complete_set_agentic_results = evaluate_qa(test_queries, multi_lm_responses, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bar graph plotting the accuracies of single-call and multi-call with 5 maximum rounds of iterative refinement on the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FM-bY4E9lhSz"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Data for plotting\n",
    "models = ['Single-Model', 'Multi-LM Agent']\n",
    "accuracies = [complete_set_single_model_accuracy, complete_set_agentic_accuracy]\n",
    "\n",
    "# Create bars\n",
    "bars = plt.bar(models, accuracies)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Comparison of Single-Model vs Multi-LM Agent Performance', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(0, 1.0)  # Set y-axis from 0 to 1\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2%}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WkHNInVlhSz"
   },
   "source": [
    "## Part 4 - Deep Research Agent [20 bonus points]\n",
    "\n",
    "Agentic LM systems are used everywhere today! From chatbots to coding agents to task automation, they are becoming more and more prevalent in our daily lives. Recently, OpenAI released \"Deep Research\", an reasoning LM agent capable of synthesizing large amounts of online information and completing multi-step research tasks: [Introducing Deep Research](https://openai.com/index/introducing-deep-research/). Google also released a similar product [Deep Research](https://blog.google/products/gemini/google-gemini-deep-research/) in December.\n",
    "\n",
    "Using the components we've built and extending them if needed, implement your own deep research agent that can generate comprehensive analyses from online sources. The agent should be able to handle complex queries requiring multi-step research, synthesizing information from multiple sources, and generating a comprehensive final report.\n",
    "\n",
    "**Key requirements for implementation:**\n",
    "- Generate a four-five paragraph report\n",
    "- Proper, easy-to-read structuring of the report\n",
    "- Usage of multiple sources of information with appropriate link citations\n",
    "- Track temporal information and maintain chronological accuracy\n",
    "\n",
    "Deliverable: In the `cs329_hw2/deep_research_agent.py` file, implement the `research` method in the `DeepResearchAgent` class.\n",
    "\n",
    "The method should:\n",
    "- Take a complex query (e.g., \"What was the UK's macroeconomic performance in 2024?\")\n",
    "- Break it down into sub-questions\n",
    "- Research each sub-question using the search engine API\n",
    "- Synthesize and summarize findings with appropriate formatting as a report\n",
    "- Return a report and list of sources \n",
    "- **IMPORTANT**: Make sure to use cheaper models during the development process to help you preserve inference compute credits and speed up the process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qT78-YulhSz"
   },
   "outputs": [],
   "source": [
    "from cs329_hw2.DeepResearchAgent import DeepResearchAgent\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the key developments and challenges in solid-state battery technology for electric vehicles in 2024, including major company announcements and technical breakthroughs?\",\n",
    "    \"How has the implementation of the UK's post-Brexit immigration policy affected its labor market and key industries between 2021-2024? Include specific policy changes and their measured impacts.\",\n",
    "    \"What progress has been made in nuclear fusion energy in 2024, focusing on major research milestones, private sector investments, and timeline predictions for commercial viability?\"\n",
    "]\n",
    "\n",
    "custom_agent = DeepResearchAgent(api_manager)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    single_LM_response = custom_agent.generate(query, model=\"gpt-4o-mini\")\n",
    "    report = custom_agent.research(query)\n",
    "    print(f\"Single-LM Response: {single_LM_response}\")\n",
    "    print(\"\\n\" * 3)\n",
    "    print(f\"Report: {report['report']}\")\n",
    "    print(f\"Sources: {report['sources']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (cs329a-hw2)",
   "language": "python",
   "name": "cs329a-hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
